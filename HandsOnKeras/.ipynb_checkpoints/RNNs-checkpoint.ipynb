{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa2be446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv1D, GRU, LSTM, TimeDistributed,SimpleRNN, Flatten, Dense, Input, InputLayer, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Activation, GlobalAvgPool2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a7f77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time=np.linspace(0,1,n_steps)\n",
    "    series = 0.5 * np.sin((time-offsets1)*(freq1*10+10))\n",
    "    series+= 0.2 * np.sin((time-offsets2)*(freq2*20+20))\n",
    "    series+= 0.1 * (np.random.rand(batch_size, n_steps)-0.5)\n",
    "    \n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9441e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=50\n",
    "series = generate_time_series(10000, n_steps +1)\n",
    "xt, yt = series[:7000, :n_steps],series[:7000, -1]\n",
    "xv, yv = series[7000:9000, :n_steps],series[7000:9000, -1]\n",
    "xte, yte = series[9000:, :n_steps],series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "258e3ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020060375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = xv[:,-1]\n",
    "#niave solution\n",
    "np.mean(keras.losses.mean_squared_error(yv, yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a330d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 0s - loss: 0.0043 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "32/32 [==============================] - 0s 665us/step - loss: 0.0040 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.003953568171709776, 0.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Niave dense\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=[50,1]),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(xt, yt, epochs = 20, validation_data = (xv, yv),verbose=0)\n",
    "model.evaluate(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e0d0484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0031104895751923323"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mse\")\n",
    "history = model.fit(xt, yt, epochs = 20, validation_data = (xv, yv),verbose=0)\n",
    "model.evaluate(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15729ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 0s - loss: 0.0119WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009826026856899261"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps=50\n",
    "series = generate_time_series(10000, n_steps +10)\n",
    "xt, yt = series[:7000, :n_steps],series[:7000, -10:,0]\n",
    "xv, yv = series[7000:9000, :n_steps],series[7000:9000,  -10:,0]\n",
    "xte, yte = series[9000:, :n_steps],series[9000:,  -10:,0]\n",
    "\n",
    "model = Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mse\")\n",
    "history = model.fit(xt, yt, epochs = 20, validation_data = (xv, yv),verbose=0)\n",
    "model.evaluate(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15979e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function last_time_step_mse at 0x0000020918DFE820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function last_time_step_mse at 0x0000020918DFE820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1458 - last_time_step_mse: 0.1444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14578290283679962, 0.14437532424926758]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps=50\n",
    "series = generate_time_series(10000, n_steps +10)\n",
    "Y = np.empty((10000,n_steps,10))\n",
    "for step in range(1,10+1):\n",
    "    Y[:,:,step-1] = series[:,step:step+n_steps,0]\n",
    "    \n",
    "Yt  = Y[:7000]\n",
    "Yv  = Y[7000:9000]\n",
    "Yte = Y[9000:]\n",
    "\n",
    "model = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    TimeDistributed(Dense(10))\n",
    "])\n",
    "\n",
    "def last_time_step_mse(Y,Yp):\n",
    "    return keras.metrics.mean_squared_error(Y[:,-1],Yp[:,-1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss = \"mse\", optimizer = optimizer, metrics = [last_time_step_mse])\n",
    "history = model.fit(xt, Yt, epochs = 20, validation_data = (xv, Yv),verbose=0)\n",
    "model.evaluate(xte, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "737ab139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14576464891433716"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    LSTM(20, return_sequences=True),\n",
    "    TimeDistributed(Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mse\")\n",
    "history = model.fit(xt, Yt, epochs = 20, validation_data = (xv, Yv),verbose=0)\n",
    "model.evaluate(xte, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46fe2d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1458585411310196"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input((None,1)),\n",
    "    Conv1D(filters=20, kernel_size=4, strides=2, padding =\"valid\"),\n",
    "    GRU(20, return_sequences=True),\n",
    "    GRU(20, return_sequences=True),\n",
    "    TimeDistributed(Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mse\")\n",
    "history = model.fit(xt, Yt[:,3::2], epochs = 20, validation_data = (xv, Yv[:,3::2]),verbose=0)\n",
    "model.evaluate(xte, Yte[:,3::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cbdc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1458 - last_time_step_mse: 0.1446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14576847851276398, 0.14456404745578766]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((None,1)))\n",
    "for rate in (1,2,4,8) * 2:\n",
    "    model.add(Conv1D(filters=20, kernel_size=2, padding =\"causal\", activation=\"relu\",dilation_rate=rate))  \n",
    "model.add(Conv1D(filters=10, kernel_size=1))\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = \"adam\", metrics =[last_time_step_mse])\n",
    "history = model.fit(xt, Yt, epochs = 20, validation_data = (xv, Yv),verbose=0)\n",
    "model.evaluate(xte, Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbe416",
   "metadata": {},
   "source": [
    "**Natural Language Processing with RNNs and Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e443dbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://homl.info/shakespeare\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\",shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "883a4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6126beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = tokenizer.document_count\n",
    "[encoded]=np.array(tokenizer.texts_to_sequences([shakespeare_text]))-1\n",
    "train_size = dataset_size*90//100\n",
    "dataset=tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b713e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=100\n",
    "window_length = n_steps +1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a31f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0841c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x0000020924FCB280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x0000020924FCB280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset=dataset.map(lambda windows: (windows[:,:-1],windows[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a41bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
